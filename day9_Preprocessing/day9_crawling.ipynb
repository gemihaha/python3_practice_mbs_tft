{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifleSoup4 를 이용한 웹페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 모듈을 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 텍스트 라인은 샘플 HTML 문서이다. 실제로는 저장된 HTML파일을 불러오거나 웹에서 바로 HTML문서를 다운로드 받아 데이터 처리를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"<html><body><h1>Mr. Belvedere Fan Club</h1><div id='nav'>navigation bar</div><div class='nav'>navigation class</div><div class='header'><a href='twitter_anywhere'>my twitter</a></div></body></html>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup의 \"html.parser\"를 이용하여 문서를 parsing한다. \"html.parser\" 외에 \"xml\", \"html5lib\" 등의 parser를 제공한다.\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><h1>Mr. Belvedere Fan Club</h1><div id=\"nav\">navigation bar</div><div class=\"nav\">navigation class</div><div class=\"header\"><a href=\"twitter_anywhere\">my twitter</a></div></body></html>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsing된 HTML 문서를 보기 좋게 보여준다.\n",
    "\n",
    "range를 사용하여 원하는 만큼만 볼 수 있다. soup.prettify()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <h1>\n",
      "   Mr. Belvedere Fan Club\n",
      "  </h1>\n",
      "  <div id=\"nav\">\n",
      "   navigation bar\n",
      "  </div>\n",
      "  <div class=\"nav\">\n",
      "   navigation class\n",
      "  </div>\n",
      "  <div class=\"header\">\n",
      "   <a href=\"twitter_anywhere\">\n",
      "    my twitter\n",
      "   </a>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문서에서 ```<h1>``` 인 요소들만 찾아 list로 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>Mr. Belvedere Fan Club</h1>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = soup.find_all(\"h1\")\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get_text'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0513ac8deec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mheading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1883\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 1884\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1885\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get_text'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "heading.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요소의 내용(text)를 반환하기 위해서 get_text()를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Belvedere Fan Club'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<질문> ```heading.get_text()``` 는 에러가 나는 이유는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"nav\">navigation bar</div>,\n",
       " <div class=\"nav\">navigation class</div>,\n",
       " <div class=\"header\"><a href=\"twitter_anywhere\">my twitter</a></div>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 요소 중, class와 id 등으로 filtering 하기 위해서 두번째 패러미터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"nav\">navigation class</div>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"nav\">navigation bar</div>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", id=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"header\"><a href=\"twitter_anywhere\">my twitter</a></div>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"header\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my twitter']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = []\n",
    "for div in divs:\n",
    "    if div.a[\"href\"] == \"twitter_anywhere\":  # 'a' 는 'a href' 의 'a'\n",
    "        id_list.append(div.a.text) # text 는 get_text()와 동일하게 사용됨.\n",
    "id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL 가져와서 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\", context=context) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_=\"tit_thumb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "황교안 \"수치를 수치로 모르면 국민이 대통령을 수치로 여겨\"\n",
      "[단독]\"대통령이 유족 보듬어 줬으면 하는 것 뿐인데\"..천안함 고 임재엽 상사 어머니의 쓴소리\n",
      "\"경유값 올리고 보조금 폐지\"..미세먼지 파격 대책 논란\n",
      "조양호 한진그룹 회장 별세\n",
      "\"이재명 친형, 2012년부터 정신병 증세\"..의사소견서 발견\n",
      "조양호 회장 미국서 별세..\"최근 폐질환 병세 급속 악화\"(종합)\n",
      "이언주 \"바른미래당으로 출마 생각할 사람 누가 있나\"\n",
      "[단독] 홍문표 의원, 4급 보좌관에 사돈 채용 논란\n",
      "文, 김연철·박영선 임명안 재가..오후 2시 장관 5명 임명식(종합)\n",
      "[단독]주영훈 靑경호처장, 경호처 무기계약직 女직원 가사도우미로 썼다\n",
      "'류여해는 무당' 발언 김동호 목사, 대법서 승소 확정\n",
      "文대통령 \"청문회 험난..능력 보여달라\" 새 장관들에 '깨알당부'\n",
      "文, 오늘 김연철·박영선 장관 임명 강행..한국당 \"파국\" 경고\n",
      "국회 찾은 윤지오 \"법 위에 선 사람들에게서 절 구원해주신 것\"\n",
      "한국당 \"강원 산불 원인은 탈원전..정책 중단 촉구\"(종합)\n",
      "'장관 인사책임론' 조국·조현옥 경질 반대 50.1%..찬성 39.4%\n",
      "소화전 앞 BMW 박살냈다..'속 시원한' 해외 소방차\n",
      "文대통령, 野 반발에도 朴·金 임명 강행..정국주도권 사수 의지\n",
      "한국당 \"독불장군식 내 사람 챙기기\"..산불 '인재' 가능성 제기\n",
      "전두환 \"고 조비오 신부는 거짓말쟁이라 쓴 건 문학적 표현\" 주장\n",
      "[인터뷰] 속초시장 \"무조건 죄송..정치쟁점화는 가슴아파\"\n",
      "리얼미터 \"文 대통령 지지율 소폭 하락..47%대 유지\"\n",
      "민주 \"정책적 능력 발휘 기대\" vs 한국 \"국민 무시 인사 만행\"\n",
      "윤지오 \"뉴시스 기자님 오셨나요?\"..법적대응 예고\n",
      "'조양호 장남' 조원태, 6월 한진그룹 경영 선포할 듯\n",
      "'땅콩·물컵 갑질'이 불러온 대한항공의 비극..한진家 수난사(종합)\n",
      "하수처리도 안되는 마을에..의원들, 빌라 짓고 도로 깔고\n",
      "평화당, 정의당과 교섭단체 구성 갈등..찬성 3, 반대 5, 유보 8\n",
      "자산 30兆 한진그룹..조양호 회장 지분가치 하루새 595억 늘어(종합)\n",
      "박원순 \"층고·용적률 상향요구 빗발..이게 서울의 미래냐\"\n",
      "이재명 '형님 정신병' 입증 스모킹건 제출\n",
      "\"YG·JYP 연습생 8년..38kg로 살빼고 男과 택시 탑승금지\"\n",
      "\"남아공서 밀렵꾼 참변..코끼리 공격에 숨지고 사자에 잡아먹혀\"\n",
      "조양호 '폐질환' 왜 공개하지 않았나..병 숨기고 미국서 치료(종합)\n",
      "[비하인드 뉴스] \"산불 정부\" 발언으로 정치공세? 김문수의 불장난\n",
      "황교안 \"박영선·김연철 임명 강행 땐 결사 각오로 저항\"\n",
      "[단독] 마이크로닷·산체스 부모, 오늘 귀국..공항서 체포될듯\n",
      "'위기 현실화' 르노삼성, 공장 가동 중단 검토..고난행보 시작되나\n",
      "'젓가락으로 햄버거 먹기' 버거킹 광고, 인종차별 논란\n",
      "홍영표 \"박영선·김연철 인격모독한 한국당에 끝까지 법적책임\"\n",
      "[단독]\"친구딸 성폭행 누명 썼다\"며 난민신청..거짓이었다\n",
      "[단독] 톱스타 여배우, 강원 화재 피해 복구에 1억 익명 기부\n",
      "숱한 위기 넘겼지만 가족사에 무릎..조양호 회장, 별세\n",
      "나경원 \"재해 추경만 분리 제출하면 초스피드 심사·통과\"\n",
      "[강원도 산불] 국가가 움직였다..달랐던 재난 대응 '세장면'\n",
      "황교안 \"朴·金 장관 임명 강행은 독선·오만·불통 정권 자인\"\n",
      "'김학의 재수사' 권고 진상조사단 둘러싸고 잇딴 파열음\n",
      "文대통령 박영선·김연철 임명강행, 찬성 45.8% vs 반대 43.3%[리얼미터]\n",
      "탁현민 \"야당과 종편의 '쑈' 놀라워..연출이 아니라 사기\"\n",
      "'라디오쇼' 류필립 \"지금 버는 돈 없다, 생활은 미나 카드로\"\n",
      "여성용 보정 속옷에서도 라돈 다량 방출\n",
      "'조양호 별세' 비상체제 한진그룹, 경영권 관건은 상속세 2000억 \n"
     ]
    }
   ],
   "source": [
    "for strong in strongs:\n",
    "#    print(strong)\n",
    "    print(strong.a.text)\n",
    "#    print(strong.a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1: 데이터 수집을 위한 리스트 작성\n",
    "\n",
    "위의 주소에서 수집하고자 하는 URL의 리스트를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://v.media.daum.net/v/20190408085630773\n",
      "http://v.media.daum.net/v/20190408060105072\n",
      "http://v.media.daum.net/v/20190408174945221\n",
      "http://v.media.daum.net/v/20190408085454721\n",
      "http://v.media.daum.net/v/20190408115242575\n",
      "http://v.media.daum.net/v/20190408113925820\n",
      "http://v.media.daum.net/v/20190408090751252\n",
      "http://v.media.daum.net/v/20190408130103619\n",
      "http://v.media.daum.net/v/20190408153741089\n",
      "http://v.media.daum.net/v/20190408120009908\n",
      "http://v.media.daum.net/v/20190408154539425\n",
      "http://v.media.daum.net/v/20190408073007047\n",
      "http://v.media.daum.net/v/20190408161433645\n",
      "http://v.media.daum.net/v/20190408050129695\n",
      "http://v.media.daum.net/v/20190408155913925\n",
      "http://v.media.daum.net/v/20190408180028571\n",
      "http://v.media.daum.net/v/20190408112914114\n",
      "http://v.media.daum.net/v/20190408091800617\n",
      "http://v.media.daum.net/v/20190408163655519\n",
      "http://v.media.daum.net/v/20190408192400887\n",
      "http://v.media.daum.net/v/20190408185947583\n",
      "http://v.media.daum.net/v/20190408122250665\n",
      "http://v.media.daum.net/v/20190408100256656\n",
      "http://v.media.daum.net/v/20190408175526423\n",
      "http://v.media.daum.net/v/20190408170233555\n",
      "http://v.media.daum.net/v/20190408153153780\n",
      "http://v.media.daum.net/v/20190408050107684\n",
      "http://v.media.daum.net/v/20190408140525540\n",
      "http://v.media.daum.net/v/20190408151219823\n",
      "http://v.media.daum.net/v/20190408165221065\n",
      "http://v.media.daum.net/v/20190407214613011\n",
      "http://v.media.daum.net/v/20190408172801432\n",
      "http://v.media.daum.net/v/20190408100314680\n",
      "http://v.media.daum.net/v/20190408111657336\n",
      "http://v.media.daum.net/v/20190408095246156\n",
      "http://v.media.daum.net/v/20190408050003645\n",
      "http://v.media.daum.net/v/20190408100428731\n",
      "http://v.media.daum.net/v/20190408161947887\n",
      "http://v.media.daum.net/v/20190408113207341\n",
      "http://v.media.daum.net/v/20190408070132665\n",
      "http://v.media.daum.net/v/20190408101814415\n",
      "http://v.media.daum.net/v/20190408092451838\n",
      "http://v.media.daum.net/v/20190408104333646\n",
      "http://v.media.daum.net/v/20190408174950223\n",
      "http://v.media.daum.net/v/20190408095048069\n",
      "http://v.media.daum.net/v/20190408173903858\n",
      "http://v.media.daum.net/v/20190408093004073\n",
      "http://v.media.daum.net/v/20190408112357777\n",
      "http://v.media.daum.net/v/20190408180202606\n",
      "http://v.media.daum.net/v/20190408143116695\n",
      "http://v.media.daum.net/v/20190408200601451\n",
      "http://v.media.daum.net/v/20190408195910373\n"
     ]
    }
   ],
   "source": [
    "url_list = []\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\", context=context) as url:\n",
    "    # 아래에 코드 작성\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_ = \"tit_thumb\")\n",
    "        \n",
    "url_list\n",
    "\n",
    "for strong in strongs:\n",
    "#    print(strong)\n",
    "#    print(strong.a.text)\n",
    "    print(strong.a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2: 기사 수집 1\n",
    "\n",
    "#### 주어진 URL의 기사의 타이틀을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나경원 \"역사 부인하는 아베 발언 치졸..사과부터 하라\"\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url, context=context) as url:\n",
    "    #아래에 코드 작성\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    h3s = soup.find_all(\"h3\" , class_=\"tit_view\")\n",
    "    \n",
    "\n",
    "\n",
    "for h3 in h3s : \n",
    "    print(h3.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언론사 이름을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴시스\n",
      "뉴시스\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url, context=context) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    cp = soup.find_all(\"em\", class_=\"info_cp\")[0]\n",
    "    print(cp.a.img['alt'])\n",
    "    print(cp.img[\"alt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
