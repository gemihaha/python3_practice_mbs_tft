{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifleSoup4 를 이용한 웹페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 모듈을 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 텍스트 라인은 샘플 HTML 문서이다. 실제로는 저장된 HTML파일을 불러오거나 웹에서 바로 HTML문서를 다운로드 받아 데이터 처리를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"<html><body><h1>Mr. Belvedere Fan Club</h1><div id='nav'>navigation bar</div><div class='nav'>navigation class</div><div class='header'><a href='twitter_anywhere'>my twitter</a></div></body></html>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup의 \"html.parser\"를 이용하여 문서를 parsing한다. \"html.parser\" 외에 \"xml\", \"html5lib\" 등의 parser를 제공한다.\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><h1>Mr. Belvedere Fan Club</h1><div id=\"nav\">navigation bar</div><div class=\"nav\">navigation class</div><div class=\"header\"><a href=\"twitter_anywhere\">my twitter</a></div></body></html>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsing된 HTML 문서를 보기 좋게 보여준다.\n",
    "\n",
    "range를 사용하여 원하는 만큼만 볼 수 있다. soup.prettify()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <h1>\n",
      "   Mr. Belvedere Fan Club\n",
      "  </h1>\n",
      "  <div id=\"nav\">\n",
      "   navigation bar\n",
      "  </div>\n",
      "  <div class=\"nav\">\n",
      "   navigation class\n",
      "  </div>\n",
      "  <div class=\"header\">\n",
      "   <a href=\"twitter_anywhere\">\n",
      "    my twitter\n",
      "   </a>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문서에서 ```<h1>``` 인 요소들만 찾아 list로 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>Mr. Belvedere Fan Club</h1>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = soup.find_all(\"h1\")\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'heding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ace4e4e4631e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mheding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'heding' is not defined"
     ]
    }
   ],
   "source": [
    "heding.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요소의 내용(text)를 반환하기 위해서 get_text()를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Belvedere Fan Club'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<질문> ```heading.get_text()``` 는 에러가 나는 이유는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"nav\">navigation bar</div>,\n",
       " <div class=\"nav\">navigation class</div>,\n",
       " <div class=\"header\"><a href=\"twitter_anywhere\">my twitter</a></div>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 요소 중, class와 id 등으로 filtering 하기 위해서 두번째 패러미터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"nav\">navigation class</div>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"nav\">navigation bar</div>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", id=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"header\"><a href=\"twitter_anywhere\">my twitter</a></div>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"header\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my twitter']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_list = []\n",
    "for div in divs:\n",
    "    if div.a[\"href\"] == \"twitter_anywhere\":  # 'a' 는 'a href' 의 'a'\n",
    "        id_list.append(div.a.text) # text 는 get_text()와 동일하게 사용됨.\n",
    "id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL 가져와서 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\", context=context) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_=\"tit_thumb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이부진 호텔신라 사장, 프로포폴 상습 투약 의혹..H성형외과 前 직원 폭로\n",
      "한국당, 文대통령 김학의사건 수사지시에 '황운하 특검' 맞불\n",
      "윤소하 원내대표 연설중 퇴장하는 자유한국당\n",
      "\"망언 도넘었다\" 나경원 사무실 점거·연행 대학생들 풀려나\n",
      "유기준 \"文, 김정은 차량 탑승 제재 위반\" vs 이낙연 \"위반 아냐\"\n",
      "\"기관총 사망자 47명\" 첫 확인..또 뚫린 '전두환 거짓말'\n",
      "아마추어에 맡긴 의전, 잇단 '외교 망신' 불러\n",
      "\"한국TV 당장 빼라\"\"관세 300% 매겨라\"..심상찮은 혐한\n",
      "강경화 \"文대통령 외교적 결례, 외교부 아픈 실수..사죄드려\"\n",
      "'이낙연'을 뚫어라..대정부질문 '불꽃' 공방\n",
      "[TEN 초점] 굳게 닫은 이미숙의 입..침묵만이 능사인가\n",
      "\"친일파 발언 규탄\" 나경원 지역구 사무실 점거 6명 연행(종합)\n",
      "'의혹 부인' 윤중천, 6년 전엔 \"김학의와 서로 영상 찍어줘\"\n",
      "\"살려달라\" 작은소리에 기지 발휘..성범죄자 잡은 시민들\n",
      "[단독] 처음으로 입 연 한화 이용규 \"타순, 옵션 불만 때문 아니다\"\n",
      "김학의 사건 개입 의혹에..황교안 \"비겁한 음해..악한 세력 존재\"\n",
      "김성태 딸만 지원서 '인편 제출'..\"절차 무시한 특혜\"\n",
      "연설 3분 만에 한국당 집단퇴장..\"내로남불\" 공방\n",
      "태극마크 붙이고.. 시리아서 IS와 싸운 한국인 있었다\n",
      "[단독]이상화♥강남 커플 '연내 결혼' 가시화, 양가 부모에 인사\n",
      "\"우리 말 안 듣더니 이제야..국가가 책임지라\"\n",
      "\"김지은 측 증인, 위증했다\" 안희정 검찰고소→무혐의\n",
      "국민연금 받다 일찍 숨지면 연금 끝,이런 일 없게 일시금 지급 추진\n",
      "수색도 체포도 거부.. 檢 '김학의 수사' 틈만 나면 뭉갰다\n",
      "'의원 늘어나도 좋은가' 한국당 현수막에..'거짓말 맙시다'\n",
      "\"한국인 싫다\" 공항 난동 일본인, 알고 보니 후생성 간부\n",
      "FC서울 출신 안델손, \"한국행 후회, 일본이 수준 더 높아\"\n",
      "\"1주년 케이크입니다\"\"단 것 잘 못먹습니다\"..文·아베 궁합 이렇다\n",
      "박정아 측, 성형설 부인 \"부기 덜 빠졌다, 몸무게 +10\" [공식]\n",
      "[단독] 갈수록 할 말 없는 '강남서'..유착 경찰관 '5명' 넘어\n",
      "[단독] \"文대통령 말레이시아서 인사말 4차례 실수\"\n",
      "땅 속 흐르고 있는 물 6천 톤..누가 언제 다 빼내나\n",
      "10개월 주입 물 1만t에 단층 뒤틀려.. 학계도 놀라게 한 포항지진\n",
      "채용홈피 두고 '지원서 인편 제출' 김성태 해명..검찰 \"확인중\"\n",
      "\"어, 한국 모텔 같은데\"..제보로 '몰카 생중계' 잡았다\n",
      "정의당, 윤소하 연설 '집단퇴장' 한국당에 \"소인배 행태\"\n",
      "피겨 임은수, 美 머라이어 벨 스케이트 날에 찍혀..\"고의적 괴롭힘\"\n",
      "전문가도 두 동강..\"홍수·가뭄 대비\" \"시궁창 냄새 어쩌나\"\n",
      "[팩트체크K] 문 대통령, 말레이 총리에 인니어 인사말..외교 결례?\n",
      "中 지하철에서 짙은 화장 여성 탑승 제지..'고스족' 집단 반발\n",
      "\"버닝썬 '무삭제 성인물' 보세요\".. 웃음거리 된 성범죄\n",
      "하루 이자 2천500만원..\"레고랜드, 제2의 알펜시아 전락 우려\"\n",
      "승리 \"20억 사기꾼에 당했다\"..국세청은 YG 세무조사 착수\n",
      "'신혼집은 남자가'는 옛말..미혼여성 72% \"동의 안해\"\n",
      "전희경 \"공수처법 결단코 막겠다, 피해자는 현재의 야당\"\n",
      "결혼에 부정적인 미혼자↑..\"결혼필요\" 男 50%·女 29%\n",
      "서열 1~3위가 대행.. 비어가는 세계최강 美軍사령부\n",
      "靑 \"검사출신 곽상도, 증거로 말해라..대통령 가족 특혜없어\"\n",
      "'생활비·논문표절 지적' 부메랑 맞은 박영선\n",
      "'눈이 부시게' 제작진·김혜자에게 시청자 찬사 쏟아지는 까닭\n",
      " 정병국 \"선거제 패스트트랙 불협화음, 한국당 때문\"\n",
      " '색'을 빼니 '속'이 확 보인다..투명 마케팅\n"
     ]
    }
   ],
   "source": [
    "for strong in strongs:\n",
    "#    print(strong)\n",
    "    print(strong.a.text)\n",
    "#    print(strong.a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1: 데이터 수집을 위한 리스트 작성\n",
    "\n",
    "위의 주소에서 수집하고자 하는 URL의 리스트를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://v.media.daum.net/v/20190320193757596\n",
      "http://v.media.daum.net/v/20190320125405326\n",
      "http://v.media.daum.net/v/20190320151039181\n",
      "http://v.media.daum.net/v/20190320225621488\n",
      "http://v.media.daum.net/v/20190320165126875\n",
      "http://v.media.daum.net/v/20190320202216506\n",
      "http://v.media.daum.net/v/20190321030105174\n",
      "http://v.media.daum.net/v/20190321060051953\n",
      "http://v.media.daum.net/v/20190320184815553\n",
      "http://v.media.daum.net/v/20190320201909437\n",
      "http://v.media.daum.net/v/20190320173425089\n",
      "http://v.media.daum.net/v/20190320201332335\n",
      "http://v.media.daum.net/v/20190320204120826\n",
      "http://v.media.daum.net/v/20190320204818952\n",
      "http://v.media.daum.net/v/20190320160219965\n",
      "http://v.media.daum.net/v/20190320191603229\n",
      "http://v.media.daum.net/v/20190320210004110\n",
      "http://v.media.daum.net/v/20190320201609380\n",
      "http://v.media.daum.net/v/20190320105439949\n",
      "http://v.media.daum.net/v/20190321030737410\n",
      "http://v.media.daum.net/v/20190320194909828\n",
      "http://v.media.daum.net/v/20190320211856435\n",
      "http://v.media.daum.net/v/20190321033610609\n",
      "http://v.media.daum.net/v/20190321060105975\n",
      "http://v.media.daum.net/v/20190320215003805\n",
      "http://v.media.daum.net/v/20190320204210847\n",
      "http://v.media.daum.net/v/20190320094005512\n",
      "http://v.media.daum.net/v/20190320154617220\n",
      "http://v.media.daum.net/v/20190321013045822\n",
      "http://v.media.daum.net/v/20190320201010276\n",
      "http://v.media.daum.net/v/20190320192117300\n",
      "http://v.media.daum.net/v/20190320194619767\n",
      "http://v.media.daum.net/v/20190321030616375\n",
      "http://v.media.daum.net/v/20190320161048421\n",
      "http://v.media.daum.net/v/20190321060031916\n",
      "http://v.media.daum.net/v/20190320172631671\n",
      "http://v.media.daum.net/v/20190320180244333\n",
      "http://v.media.daum.net/v/20190321000945310\n",
      "http://v.media.daum.net/v/20190320214430748\n",
      "http://v.media.daum.net/v/20190320165701142\n",
      "http://v.media.daum.net/v/20190320143023909\n",
      "http://v.media.daum.net/v/20190320154129913\n",
      "http://v.media.daum.net/v/20190321000844288\n",
      "http://v.media.daum.net/v/20190321053007376\n",
      "http://v.media.daum.net/v/20190320200901251\n",
      "http://v.media.daum.net/v/20190320181648843\n",
      "http://v.media.daum.net/v/20190321030903428\n",
      "http://v.media.daum.net/v/20190320150636025\n",
      "http://v.media.daum.net/v/20190320154050871\n",
      "http://v.media.daum.net/v/20190320191517215\n",
      "http://v.media.daum.net/v/20190321085700752\n",
      "http://v.media.daum.net/v/20190321085334689\n"
     ]
    }
   ],
   "source": [
    "url_list = []\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\", context=context) as url:\n",
    "    # 아래에 코드 작성\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_ = \"tit_thumb\")\n",
    "        \n",
    "url_list\n",
    "\n",
    "for strong in strongs:\n",
    "#    print(strong)\n",
    "#    print(strong.a.text)\n",
    "    print(strong.a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2: 기사 수집 1\n",
    "\n",
    "#### 주어진 URL의 기사의 타이틀을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나경원 \"역사 부인하는 아베 발언 치졸..사과부터 하라\"\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url, context=context) as url:\n",
    "    #아래에 코드 작성\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    h3s = soup.find_all(\"h3\" , class_=\"tit_view\")\n",
    "    \n",
    "\n",
    "\n",
    "for h3 in h3s : \n",
    "    print(h3.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언론사 이름을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴시스\n",
      "뉴시스\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url, context=context) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    cp = soup.find_all(\"em\", class_=\"info_cp\")[0]\n",
    "    print(cp.a.img['alt'])\n",
    "    print(cp.img[\"alt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
